{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jlXftOLAKW_4"
   },
   "source": [
    "## RL Games"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "id": "4PPdGubXmKG9"
   },
   "outputs": [],
   "source": [
    "from IPython.display import clear_output\n",
    "\n",
    "!pip install gym\n",
    "!pip install nqueens\n",
    "!pip install -U ray\n",
    "\n",
    "!gdown --id 1lXBaqqOzMnmr7F04T9H2UnbTyN9g4Pjs\n",
    "!gdown --id 1TTByOPj9JvBGS6I5B71u5qxaKKIXB9JZ\n",
    "!gdown --id 1SPeH3NlXETsdVzRTV0YqwWRwcRmUMgao\n",
    "\n",
    "!mkdir /content/save\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "id": "1bTDB4hEmPSr"
   },
   "outputs": [],
   "source": [
    "import gym\n",
    "from gym import Env\n",
    "from gym.spaces import Discrete, Box, MultiDiscrete\n",
    "from nqueens import Queen\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import tqdm\n",
    "import os\n",
    "import gc\n",
    "import copy\n",
    "\n",
    "import cv2\n",
    "from google.colab.patches import cv2_imshow\n",
    "\n",
    "\n",
    "from IPython.display import HTML\n",
    "from collections import namedtuple, deque\n",
    "from itertools import count\n",
    "from base64 import b64encode\n",
    "\n",
    "\n",
    "from IPython import display"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fABA6t4wmRTV"
   },
   "source": [
    "## Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "id": "qq27G6JDmQbQ"
   },
   "outputs": [],
   "source": [
    "class Vasuki(Env):\n",
    "\n",
    "    def _food_position_(self, n):\n",
    "        # Using the N-Queens problem to uniformly distribute the food spawning location\n",
    "        qq = Queen(n)\n",
    "        food_pos = np.empty(shape = [0, 2])\n",
    "        chess = qq.queen_data[0]\n",
    "        for x in range(n):\n",
    "            for y in range(n):\n",
    "                if chess[y][x] == 1:\n",
    "                    arr = np.array([[x, y]])\n",
    "                    food_pos = np.append(food_pos, arr, axis = 0)\n",
    "        # Returning the n food locations which are spatially distributed uniformly\n",
    "        return food_pos\n",
    "    \n",
    "    def _init_agent_(self, score=0):\n",
    "        # Creating a dictionary to store the information related to the agent\n",
    "        agent = {}\n",
    "        # Set initial direction of head of the Snake :  North = 0, East = 1, South = 2, West = 3\n",
    "        agent['head'] = np.random.randint(low = 0, high = 4, size = (1)).item()\n",
    "        # The score for each agent\n",
    "        agent['score'] = score\n",
    "        # Set initial position \n",
    "        agent['state'] = np.random.randint(low = 0, high = self.n, size = (2))\n",
    "        # Velocity of the snake\n",
    "        agent['velocity'] = 1            \n",
    "        # Returning the Agent Properties\n",
    "        return agent\n",
    "\n",
    "    def _init_image_(self, path):\n",
    "        # Loading the image\n",
    "        image = cv2.imread(path)\n",
    "        # Resizing the image\n",
    "        image = cv2.resize(image, (self.scale-1,self.scale-1), interpolation=cv2.INTER_NEAREST)\n",
    "        # Returning the preprocessed image\n",
    "        return image\n",
    "\n",
    "    def __init__(self, n, rewards, game_length=100):\n",
    "        # Parameters\n",
    "        self.n = n\n",
    "        self.rewards = rewards\n",
    "        self.scale = 256//self.n\n",
    "        # Actions we can take : left = 0, forward = 1, right = 2\n",
    "        self.action_space = Discrete(3)\n",
    "        # The nxn grid\n",
    "        self.observation_space = MultiDiscrete([self.n, self.n])\n",
    "        # Set Total Game length\n",
    "        self.game_length = game_length\n",
    "        self.game_length_ = self.game_length\n",
    "        # Set Food Spawning locations. Totally there are only n locations\n",
    "        self.foodspawn_space = self._food_position_(self.n)\n",
    "        # Out of the n food locations, at any time only n/2 random locations have food\n",
    "        self.live_index = np.random.choice(len(self.foodspawn_space), size=(self.n//2), replace=False)\n",
    "        self.live_foodspawn_space = self.foodspawn_space[self.live_index]\n",
    "        # Initializing the Agents\n",
    "        self.agentA = self._init_agent_()\n",
    "        self.agentB = self._init_agent_()\n",
    "        # Loading the Images\n",
    "        self.image_agentA = self._init_image_(\"agentA.png\")\n",
    "        self.image_agentB = self._init_image_(\"agentB.png\")\n",
    "        self.image_prey = self._init_image_(\"prey.png\")\n",
    "        # Creating History\n",
    "        encoded, _ = self.encode()\n",
    "        self.history = [] # {\"agentA\": self.agentA, \"agentB\":self.agentB, \"live_foodspawn_space\": self.live_foodspawn_space, 'encoded': encoded}\n",
    "\n",
    "    def _movement_(self, action, agent):\n",
    "        # Loading the states\n",
    "        illegal = 0     # If the snake hits the walls\n",
    "        n = self.n\n",
    "        head = agent['head']\n",
    "        state = agent['state'].copy()\n",
    "        velocity = agent['velocity']\n",
    "        score = agent['score']\n",
    "        # Applying the Action\n",
    "        if action == 0: # Go Left\n",
    "            if head==0:\n",
    "                if state[1]==velocity-1: # Left Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, -velocity])\n",
    "                head = 3\n",
    "            elif head==1:\n",
    "                if state[0]==velocity-1: # Top Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([-velocity, 0])\n",
    "                head = 0\n",
    "            elif head==2: \n",
    "                if state[1]==n-velocity: # Right Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, velocity])\n",
    "                head = 1\n",
    "            elif head==3:\n",
    "                if state[0]==n-velocity: # Bottom Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([velocity, 0])\n",
    "                head = 2           \n",
    "        elif action == 1: # Move Forward\n",
    "            if head==0:\n",
    "                if state[0]==velocity-1: # Top Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([-velocity, 0])\n",
    "                head = 0\n",
    "            elif head==1:\n",
    "                if state[1]==n-velocity: # Right Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, velocity])\n",
    "                head = 1\n",
    "            elif head==2:\n",
    "                if state[0]==n-velocity: # Bottom Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([velocity, 0])\n",
    "                head = 2\n",
    "            elif head==3:\n",
    "                if state[1]==velocity-1: # Left Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, -velocity])\n",
    "                head = 3\n",
    "        elif action == 2: # Go Right\n",
    "            if head==0:\n",
    "                if state[1]==n-velocity: # Right Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, velocity])\n",
    "                head = 1\n",
    "            elif head==1:\n",
    "                if state[0]==n-velocity: # Bottom Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([velocity, 0])\n",
    "                head = 2\n",
    "            elif head==2:\n",
    "                if state[1]==velocity-1: # Left Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([0, -velocity])\n",
    "                head = 3\n",
    "            elif head==3:\n",
    "                if state[0]==velocity-1: # Top Wall\n",
    "                    illegal = 1\n",
    "                    change = np.array([0, 0])\n",
    "                else:\n",
    "                    change = np.array([-velocity, 0])\n",
    "                head = 0\n",
    "        # Updating the agent properties\n",
    "        modified = {'head': head, 'state':state+change, 'score':score, 'velocity':velocity}\n",
    "        return modified, illegal\n",
    "\n",
    "    def _reward_(self, agent, illegal):\n",
    "        # Loading the states\n",
    "        head = agent['head']\n",
    "        state = agent['state'].copy()\n",
    "        velocity = agent['velocity']\n",
    "        score = agent['score']\n",
    "        # Calculating the reward\n",
    "        if illegal == 1: # If the snake hits the wall\n",
    "            reward = self.rewards['Illegal']\n",
    "        else:\n",
    "            if True in np.all((state == self.live_foodspawn_space), axis = 1):\n",
    "                # Finding the index of the state\n",
    "                index = np.where(np.all((state == self.live_foodspawn_space), axis = 1) == True)[0].item()\n",
    "                # Computing the empty foodspawn spaces\n",
    "                empty_foodspawn_space = [space for space in self.foodspawn_space if space not in self.live_foodspawn_space]\n",
    "                # Removing the state from live foodspawn space\n",
    "                self.live_foodspawn_space = np.delete(self.live_foodspawn_space, index, 0)\n",
    "                # Updating the live foodspawn space\n",
    "                addition = np.random.choice(len(empty_foodspawn_space), size=1, replace=False)\n",
    "                self.live_foodspawn_space = np.append(self.live_foodspawn_space, np.expand_dims(empty_foodspawn_space[addition.item(0)], axis = 0), axis=0)\n",
    "                assert  len(set([(x,y) for (x,y) in self.live_foodspawn_space])) == 4\n",
    "                # If the snake lands on the food\n",
    "                reward = self.rewards['Food']\n",
    "            else:\n",
    "                # If the snake just moves\n",
    "                reward = self.rewards['Movement']\n",
    "        return reward\n",
    "\n",
    "    def step(self, action):\n",
    "        actionA = action['actionA']\n",
    "        actionB = action['actionB']\n",
    "        # Applying the actions\n",
    "        self.agentA, illegalA = self._movement_(actionA, self.agentA)\n",
    "        self.agentB, illegalB = self._movement_(actionB, self.agentB) \n",
    "        # Calculating the reward\n",
    "        if (self.agentA['state'] == self.agentB['state']).all():\n",
    "            if self.agentA['score'] > self.agentB['score']:\n",
    "                rewardA = 5 * abs( self.agentB['score']//(self.agentA['score']-self.agentB['score']) )\n",
    "                rewardB = - 3 * abs( self.agentB['score']//(self.agentA['score']-self.agentB['score']) )\n",
    "                _ = self._reward_(self.agentA, illegalA)\n",
    "                score = self.agentB['score']\n",
    "                while True:\n",
    "                    self.agentB = self._init_agent_(score)\n",
    "                    if (self.agentB['state']!=self.agentA['state']).all():\n",
    "                        _ = self._reward_(self.agentB, illegalB)\n",
    "                        break\n",
    "            elif self.agentA['score'] < self.agentB['score']:\n",
    "                rewardA = - 3 * abs( self.agentA['score']//(self.agentA['score']-self.agentB['score']) )\n",
    "                rewardB = 5 * abs( self.agentA['score']//(self.agentA['score']-self.agentB['score']) )\n",
    "                _ = self._reward_(self.agentB, illegalB)\n",
    "                score = self.agentA['score']\n",
    "                while True:\n",
    "                    self.agentA = self._init_agent_(score) \n",
    "                    if (self.agentA['state']!=self.agentB['state']).all():\n",
    "                        _ = self._reward_(self.agentA, illegalA)\n",
    "                        break\n",
    "            elif self.agentA['score'] == self.agentB['score']:\n",
    "                rewardA = - abs(self.agentA['score']//2)\n",
    "                rewardB = - abs(self.agentB['score']//2)\n",
    "                while True:\n",
    "                    self.agentA = self._init_agent_(score=self.agentA['score'])\n",
    "                    if (self.agentA['state']!=self.agentB['state']).all():\n",
    "                        _ = self._reward_(self.agentA, illegalA)\n",
    "                        break\n",
    "                while True:\n",
    "                    self.agentB = self._init_agent_(score=self.agentB['score'])\n",
    "                    if (self.agentB['state']!=self.agentA['state']).all():\n",
    "                        _ = self._reward_(self.agentB, illegalB)\n",
    "                        break\n",
    "        else:\n",
    "            rewardA = self._reward_(self.agentA, illegalA)\n",
    "            rewardB = self._reward_(self.agentB, illegalB)\n",
    "        # Adding the reward to the score\n",
    "        self.agentA['score'] = self.agentA['score'] + rewardA\n",
    "        self.agentB['score'] = self.agentB['score'] + rewardB\n",
    "        # Updating history\n",
    "        encoded, _ = self.encode()\n",
    "        self.history.append({\"agentA\": self.agentA, \"agentB\":self.agentB, \"live_foodspawn_space\": self.live_foodspawn_space, \"encoded\": encoded, \n",
    "                             \"rewardA\": rewardA, \"actionA\": actionA, \"rewardB\": rewardB, \"actionB\": actionB})\n",
    "        # Check if game is done\n",
    "        self.game_length -= 1\n",
    "        if self.game_length <= 0:\n",
    "            done = True\n",
    "        else:\n",
    "            done = False\n",
    "        # Set placeholder for info\n",
    "        info = {'agentA': self.agentA, 'agentB': self.agentB}\n",
    "        return  rewardA, rewardB, done, info\n",
    "\n",
    "    def _rotate_(self, image, direction):\n",
    "        # Rotating the image to rectify the direction of the head\n",
    "        if direction == 1:\n",
    "            image = np.rot90(image.copy(), k = 3)\n",
    "        elif direction == 2: \n",
    "            image = np.rot90(image.copy(), k = 2)\n",
    "        elif direction == 3:\n",
    "            image = np.rot90(image.copy())\n",
    "        return image\n",
    "\n",
    "    def render(self, actionA, actionB): # Returns a one-hot encoded state\n",
    "        # Loading the states\n",
    "        live_foodspawn_space_ = self.history[-2][\"live_foodspawn_space\"]\n",
    "        agentA = self.history[-2][\"agentA\"]\n",
    "        agentB = self.history[-2][\"agentB\"]\n",
    "        snakeA = agentA['state']\n",
    "        snakeB = agentB['state']\n",
    "        # Initializing the state\n",
    "        state = np.ones((self.scale*self.n, 2*self.scale*self.n, 3))*255\n",
    "        # Adding grid lines\n",
    "        for x in range(self.n+1):\n",
    "            state[self.scale*x:self.scale*x+1, :self.scale*self.n] = [0, 0, 0]\n",
    "        for y in range(self.n+1):\n",
    "            state[:, self.scale*y:self.scale*y+1] = [0, 0, 0]\n",
    "        # Adding the live food location\n",
    "        assert  len(set([(x,y) for (x,y) in live_foodspawn_space_])) == 4\n",
    "        for food in live_foodspawn_space_.tolist():\n",
    "            x = int(food[0])\n",
    "            y = int(food[1])\n",
    "            state[self.scale*x+1:self.scale*x+self.scale, self.scale*y+1:self.scale*y+self.scale] = self.image_prey\n",
    "        # Annotating\n",
    "        font = cv2.FONT_HERSHEY_SIMPLEX\n",
    "        fontScale = 0.4\n",
    "        color = (0, 0, 0)\n",
    "        thickness = 1\n",
    "        direction = {0:\"North\", 1:\"East\", 2:\"South\", 3:\"West\"}\n",
    "        action = {0:\"Left\", 1:\"Forward\", 2:\"Right\", \"None\":\"None\"}\n",
    "        stateA = \"State A: [{0},{1}]\".format(snakeA[0], snakeA[1])\n",
    "        stateB = \"State B: [{0},{1}]\".format(snakeB[0], snakeB[1])\n",
    "        scoreA = \"Score A: \" + str(agentA['score'])\n",
    "        scoreB = \"Score B: \" + str(agentB['score'])\n",
    "        headA = \"Head A: \" + direction[agentA['head']]\n",
    "        headB = \"Head B: \" + direction[agentB['head']]\n",
    "        actionA = \"Action A: \" + action[actionA]\n",
    "        actionB = \"Action B: \" + action[actionB]\n",
    "        # Adding the text\n",
    "        start = 80\n",
    "        state = cv2.putText(state, scoreA, (265, start), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, stateA, (265, start+32), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, headA, (265, start+64), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, actionA, (265, start+96), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, scoreB, (390, start), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, stateB, (390, start+32), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, headB, (390, start+64), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        state = cv2.putText(state, actionB, (390, start+96), font, fontScale, color, thickness, cv2.LINE_AA)\n",
    "        # Adding the agents\n",
    "        image_agentA = self._rotate_(self.image_agentA, agentA['head'])\n",
    "        image_agentB = self._rotate_(self.image_agentB, agentB['head'])\n",
    "        state[self.scale*snakeA[0]+1:self.scale*snakeA[0]+self.scale, self.scale*snakeA[1]+1:self.scale*snakeA[1]+self.scale] = image_agentA\n",
    "        state[self.scale*snakeB[0]+1:self.scale*snakeB[0]+self.scale, self.scale*snakeB[1]+1:self.scale*snakeB[1]+self.scale] = image_agentB\n",
    "        # Returning the state\n",
    "        return state\n",
    "\n",
    "    def encode(self):\n",
    "        # Loading the states\n",
    "        encoder = {'blank': 0, 'foodspawn_space': 1, 'agentA': 2, 'agentB': 3}\n",
    "        state = np.zeros((self.n, self.n))\n",
    "        live_foodspawn_space = self.live_foodspawn_space.astype(np.int)\n",
    "        snakeA = self.agentA['state']\n",
    "        snakeB = self.agentB['state']\n",
    "        # Adding the agents and snakes\n",
    "        state[live_foodspawn_space[:,0], live_foodspawn_space[:,1]] = encoder['foodspawn_space']\n",
    "        state[snakeA[0], snakeA[1]] = encoder['agentA']\n",
    "        state[snakeB[0], snakeB[1]] = encoder['agentB']\n",
    "        # One-Hot encoding the state\n",
    "        encoded = np.eye(len(encoder.keys()))[state.astype(np.int)]\n",
    "        encoded = np.moveaxis(encoded, -1, 0)\n",
    "        # Returning the encoded and state\n",
    "        return encoded, state\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset Total Game length\n",
    "        self.game_length = self.game_length_\n",
    "        # Reset Food Spawning locations\n",
    "        self.foodspawn_space = self._food_position_(self.n)\n",
    "        # Reset Live Food Spawning locations\n",
    "        self.live_index = np.random.choice(len(self.foodspawn_space), size=(self.n//2), replace=False)\n",
    "        self.live_foodspawn_space = self.foodspawn_space[self.live_index]\n",
    "        # Reset Agents\n",
    "        self.agentA = self._init_agent_()\n",
    "        self.agentB = self._init_agent_()\n",
    "        # Clear History\n",
    "        self.history = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "BTv-xDmNtUAw"
   },
   "outputs": [],
   "source": [
    "config = {'n': 8, 'rewards': {'Food': 4, 'Movement': -1, 'Illegal': -2}, 'game_length': 100} # You can change during training but not during evaluation\n",
    "\n",
    "env = Vasuki(**config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UCkJjkpRIcFC"
   },
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Gr61vkRO8Inb"
   },
   "source": [
    "# Inferencer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "id": "XWzhi9x28Jpj"
   },
   "outputs": [],
   "source": [
    "class Runner():\n",
    "    def __init__(self, model_A, model_B, checkpoint):\n",
    "        # Path to store the Video\n",
    "        self.checkpoint = checkpoint\n",
    "        # Defining the Environment\n",
    "        config = {'n': 8, 'rewards': {'Food': 4, 'Movement': -1, 'Illegal': -2}, 'game_length': 100} # Should not change for evaluation\n",
    "        self.env = Vasuki(**config)\n",
    "        self.runs = 1000\n",
    "        # Trained Policies\n",
    "        self.model_A = model_A # Loaded model with weights\n",
    "        self.model_B = model_B # Loaded model with weights\n",
    "        # Results\n",
    "        self.winner = {'Player_A': 0, 'Player_B': 0}\n",
    "\n",
    "    def reset(self):\n",
    "        self.winner = {'Player_A': 0, 'Player_B': 0}\n",
    "\n",
    "    def evaluate_A(self):\n",
    "        # Uses self.env as the environment and returns the best action for Player A (Blue)\n",
    "        state = get_XinA(self.env)\n",
    "        action_A,_ = choose_action_epsilon_greedy(self.model_A,state,0)\n",
    "        return action_A # Action in {0, 1, 2}\n",
    "\n",
    "    def evaluate_B(self):\n",
    "        # Uses self.env as the environment and returns the best action for Player B (Red)\n",
    "        action_B = np.random.choice([0,1,2])\n",
    "        return action_B # Action in {0, 1, 2}\n",
    "\n",
    "    def visualize(self, run):\n",
    "        self.env.reset()\n",
    "        done = False\n",
    "        video = []\n",
    "        while not done:\n",
    "            # Actions based on the current state using the learned policy \n",
    "            actionA = self.evaluate_A()\n",
    "            actionB = self.evaluate_B()\n",
    "            action = {'actionA': actionA, 'actionB': actionB}\n",
    "            rewardA, rewardB, done, info = self.env.step(action)\n",
    "            # Rendering the enviroment to generate the simulation\n",
    "            if len(self.env.history)>1:\n",
    "                state = self.env.render(actionA, actionB)\n",
    "                encoded, _ = self.env.encode()\n",
    "                state = np.array(state, dtype=np.uint8)\n",
    "                video.append(state)\n",
    "        # Recording the Winner\n",
    "        if self.env.agentA['score'] > self.env.agentB['score']:\n",
    "            self.winner['Player_A'] += 1\n",
    "        elif self.env.agentB['score'] > self.env.agentA['score']:\n",
    "            self.winner['Player_B'] += 1\n",
    "        # Generates a video simulation of the game\n",
    "        if run%100==0:\n",
    "            aviname = os.path.join(self.checkpoint, f\"game_{run}.avi\")\n",
    "            mp4name = os.path.join(self.checkpoint, f\"game_{run}.mp4\")\n",
    "            w, h, _ = video[0].shape\n",
    "            out = cv2.VideoWriter(aviname, cv2.VideoWriter_fourcc(*'DIVX'), 2, (h, w))\n",
    "            for state in video:\n",
    "                assert state.shape==(256,512,3)\n",
    "                out.write(state)\n",
    "            cv2.destroyAllWindows()\n",
    "            os.popen(\"ffmpeg -i {input} {output}\".format(input=aviname, output=mp4name))\n",
    "            # os.popen(\"rm -f {input}\".format(input=aviname))\n",
    "\n",
    "    def arena(self):\n",
    "        # Pitching the Agents against each other\n",
    "        for run in range(1, self.runs+1, 1):\n",
    "            self.visualize(run)\n",
    "        return self.winner"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "BgAPq6pCmF1r"
   },
   "source": [
    "# **Helped Code/Functions Agent A**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "Z_p5kfN9kQOO"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "id": "dycNhse5kLLw"
   },
   "outputs": [],
   "source": [
    "class DQN(nn.Module):\n",
    "\n",
    "    def __init__(self, state_space_dim, action_space_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Sequential(\n",
    "                  nn.Linear(state_space_dim,64),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(64,64),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(64,32),\n",
    "                  nn.ReLU(),\n",
    "                  nn.Linear(32,action_space_dim)\n",
    "                )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.to(device)\n",
    "        return self.linear(x)\n",
    "\n",
    "def get_XinA(env):\n",
    "    '''Returns the 15 dim vector as the state input to the RL Deep Q Network'''\n",
    "    # Declaring the booleans\n",
    "    food_is_behind_of_A = 0\n",
    "    food_is_front_of_A = 0\n",
    "    food_is_left_of_A = 0\n",
    "    food_is_right_of_A = 0\n",
    "\n",
    "    B_is_near = False\n",
    "    B_is_behind_of_A_and_scoreA_more_than_scoreB = 0\n",
    "    B_is_front_of_A_and_scoreA_more_than_scoreB = 0\n",
    "    B_is_left_of_A_and_scoreA_more_than_scoreB = 0\n",
    "    B_is_right_of_A_and_scoreA_more_than_scoreB = 0\n",
    "    B_is_behind_of_A_and_scoreA_less_than_scoreB = 0\n",
    "    B_is_front_of_A_and_scoreA_less_than_scoreB = 0\n",
    "    B_is_left_of_A_and_scoreA_less_than_scoreB = 0\n",
    "    B_is_right_of_A_and_scoreA_less_than_scoreB = 0\n",
    "\n",
    "    border_is_left_of_A = 0\n",
    "    border_is_right_of_A = 0\n",
    "    border_is_front_of_A = 0\n",
    "\n",
    "    # Finding the nearest food\n",
    "    A_loc = [env.agentA[\"state\"][0], env.agentA[\"state\"][1]]\n",
    "    food_locs = env.live_foodspawn_space\n",
    "    rel_food_locs = [[loc[0] - A_loc[0], loc[1] - A_loc[1]] for loc in food_locs]\n",
    "    food_dist_from_A = [abs(loc[0]) + abs(loc[1]) for loc in rel_food_locs]\n",
    "    nearest_food_index = food_dist_from_A.index(min(food_dist_from_A))\n",
    "    nearest_food_rel_loc = rel_food_locs[nearest_food_index]\n",
    "\n",
    "    # Getting relative location of B. And proximity\n",
    "    B_loc = [env.agentB[\"state\"][0], env.agentB[\"state\"][1]]\n",
    "    rel_loc_of_B = [B_loc[0] - A_loc[0], B_loc[1] - A_loc[1]]\n",
    "    if abs(rel_loc_of_B[0]) + abs(rel_loc_of_B[1]) == 2:\n",
    "        B_is_near = True\n",
    "\n",
    "    # Considering the head direction and updating booleans\n",
    "    if env.agentA[\"head\"] == 0:\n",
    "        if nearest_food_rel_loc[0] > 0:\n",
    "            food_is_behind_of_A = 1\n",
    "        if nearest_food_rel_loc[0] < 0:\n",
    "            food_is_front_of_A = 1\n",
    "        if nearest_food_rel_loc[1] > 0:\n",
    "            food_is_right_of_A = 1\n",
    "        if nearest_food_rel_loc[1] < 0:\n",
    "            food_is_left_of_A = 1\n",
    "        if B_is_near:\n",
    "            if env.agentA[\"score\"] > env.agentB[\"score\"]:\n",
    "                if rel_loc_of_B[0] > 0:\n",
    "                    B_is_behind_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[0] < 0:\n",
    "                    B_is_front_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] > 0:\n",
    "                    B_is_right_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] < 0:\n",
    "                    B_is_left_of_A_and_scoreA_more_than_scoreB = 1\n",
    "            if env.agentA[\"score\"] <= env.agentB[\"score\"]:\n",
    "                if rel_loc_of_B[0] > 0:\n",
    "                    B_is_behind_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[0] < 0:\n",
    "                    B_is_front_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] > 0:\n",
    "                    B_is_right_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] < 0:\n",
    "                    B_is_left_of_A_and_scoreA_less_than_scoreB = 1\n",
    "        if A_loc[0] == 0:\n",
    "            border_is_front_of_A = 1\n",
    "        if A_loc[1] == 0:\n",
    "            border_is_left_of_A = 1\n",
    "        if A_loc[1] == 7:\n",
    "            border_is_right_of_A = 1\n",
    "\n",
    "    if env.agentA[\"head\"] == 1:\n",
    "        if nearest_food_rel_loc[0] > 0:\n",
    "            food_is_right_of_A = 1\n",
    "        if nearest_food_rel_loc[0] < 0:\n",
    "            food_is_left_of_A = 1\n",
    "        if nearest_food_rel_loc[1] > 0:\n",
    "            food_is_front_of_A = 1\n",
    "        if nearest_food_rel_loc[1] < 0:\n",
    "            food_is_behind_of_A = 1\n",
    "        if B_is_near:\n",
    "            if env.agentA[\"score\"] > env.agentB[\"score\"]:\n",
    "                if rel_loc_of_B[0] > 0:\n",
    "                    B_is_right_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[0] < 0:\n",
    "                    B_is_left_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] > 0:\n",
    "                    B_is_front_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] < 0:\n",
    "                    B_is_behind_of_A_and_scoreA_more_than_scoreB = 1\n",
    "            if env.agentA[\"score\"] <= env.agentB[\"score\"]:\n",
    "                if rel_loc_of_B[0] > 0:\n",
    "                    B_is_right_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[0] < 0:\n",
    "                    B_is_left_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] > 0:\n",
    "                    B_is_front_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] < 0:\n",
    "                    B_is_behind_of_A_and_scoreA_less_than_scoreB = 1\n",
    "        if A_loc[0] == 0:\n",
    "            border_is_left_of_A = 1\n",
    "        if A_loc[0] == 7:\n",
    "            border_is_right_of_A = 1\n",
    "        if A_loc[1] == 7:\n",
    "            border_is_front_of_A = 1\n",
    "\n",
    "    if env.agentA[\"head\"] == 2:\n",
    "        if nearest_food_rel_loc[0] > 0:\n",
    "            food_is_front_of_A = 1\n",
    "        if nearest_food_rel_loc[0] < 0:\n",
    "            food_is_behind_of_A = 1\n",
    "        if nearest_food_rel_loc[1] > 0:\n",
    "            food_is_left_of_A = 1\n",
    "        if nearest_food_rel_loc[1] < 0:\n",
    "            food_is_right_of_A = 1\n",
    "        if B_is_near:\n",
    "            if env.agentA[\"score\"] > env.agentB[\"score\"]:\n",
    "                if rel_loc_of_B[0] > 0:\n",
    "                    B_is_front_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[0] < 0:\n",
    "                    B_is_behind_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] > 0:\n",
    "                    B_is_left_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] < 0:\n",
    "                    B_is_right_of_A_and_scoreA_more_than_scoreB = 1\n",
    "            if env.agentA[\"score\"] <= env.agentB[\"score\"]:\n",
    "                if rel_loc_of_B[0] > 0:\n",
    "                    B_is_front_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[0] < 0:\n",
    "                    B_is_behind_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] > 0:\n",
    "                    B_is_left_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] < 0:\n",
    "                    B_is_right_of_A_and_scoreA_less_than_scoreB = 1\n",
    "        if A_loc[0] == 7:\n",
    "            border_is_front_of_A = 1\n",
    "        if A_loc[1] == 0:\n",
    "            border_is_right_of_A = 1\n",
    "        if A_loc[1] == 7:\n",
    "            border_is_left_of_A = 1\n",
    "\n",
    "    if env.agentA[\"head\"] == 3:\n",
    "        if nearest_food_rel_loc[0] > 0:\n",
    "            food_is_left_of_A = 1\n",
    "        if nearest_food_rel_loc[0] < 0:\n",
    "            food_is_right_of_A = 1\n",
    "        if nearest_food_rel_loc[1] > 0:\n",
    "            food_is_behind_of_A = 1\n",
    "        if nearest_food_rel_loc[1] < 0:\n",
    "            food_is_front_of_A = 1\n",
    "        if B_is_near:\n",
    "            if env.agentA[\"score\"] > env.agentB[\"score\"]:\n",
    "                if rel_loc_of_B[0] > 0:\n",
    "                    B_is_left_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[0] < 0:\n",
    "                    B_is_right_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] > 0:\n",
    "                    B_is_behind_of_A_and_scoreA_more_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] < 0:\n",
    "                    B_is_front_of_A_and_scoreA_more_than_scoreB = 1\n",
    "            if env.agentA[\"score\"] <= env.agentB[\"score\"]:\n",
    "                if rel_loc_of_B[0] > 0:\n",
    "                    B_is_left_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[0] < 0:\n",
    "                    B_is_right_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] > 0:\n",
    "                    B_is_behind_of_A_and_scoreA_less_than_scoreB = 1\n",
    "                if rel_loc_of_B[1] < 0:\n",
    "                    B_is_front_of_A_and_scoreA_less_than_scoreB = 1\n",
    "        if A_loc[0] == 0:\n",
    "            border_is_right_of_A = 1\n",
    "        if A_loc[0] == 7:\n",
    "            border_is_left_of_A = 1\n",
    "        if A_loc[1] == 0:\n",
    "            border_is_front_of_A = 1\n",
    "\n",
    "    return [\n",
    "        food_is_behind_of_A,\n",
    "        food_is_front_of_A,\n",
    "        food_is_left_of_A,\n",
    "        food_is_right_of_A,\n",
    "        B_is_behind_of_A_and_scoreA_more_than_scoreB,\n",
    "        B_is_front_of_A_and_scoreA_more_than_scoreB,\n",
    "        B_is_left_of_A_and_scoreA_more_than_scoreB,\n",
    "        B_is_right_of_A_and_scoreA_more_than_scoreB,\n",
    "        B_is_behind_of_A_and_scoreA_less_than_scoreB,\n",
    "        B_is_front_of_A_and_scoreA_less_than_scoreB,\n",
    "        B_is_left_of_A_and_scoreA_less_than_scoreB,\n",
    "        B_is_right_of_A_and_scoreA_less_than_scoreB,\n",
    "        border_is_left_of_A,\n",
    "        border_is_right_of_A,\n",
    "        border_is_front_of_A,\n",
    "    ]\n",
    "\n",
    "def choose_action_epsilon_greedy(net, state, epsilon):\n",
    "    \n",
    "    if epsilon > 1 or epsilon < 0:\n",
    "        raise Exception('The epsilon value must be between 0 and 1')\n",
    "                \n",
    "    # Evaluate the network output from the current state\n",
    "    with torch.no_grad():\n",
    "        net.eval()\n",
    "        state = torch.tensor(state, dtype=torch.float32) # Convert the state to tensor\n",
    "        net_out = net(state)\n",
    "\n",
    "    # Get the best action (argmax of the network output)\n",
    "    best_action = int(net_out.argmax())\n",
    "    # Get the number of possible actions\n",
    "    action_space_dim = net_out.shape[-1]\n",
    "\n",
    "    # Select a non optimal action with probability epsilon, otherwise choose the best action\n",
    "    if random.random() < epsilon:\n",
    "        # List of non-optimal actions (this list includes all the actions but the optimal one)\n",
    "        non_optimal_actions = [a for a in range(action_space_dim) if a != best_action]\n",
    "        # Select randomly from non_optimal_actions\n",
    "        action = random.choice(non_optimal_actions)\n",
    "    else:\n",
    "        # Select best action\n",
    "        action = best_action\n",
    "        \n",
    "    return action, net_out.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "id": "0oAKNt_VkXmo"
   },
   "outputs": [],
   "source": [
    "model = torch.load(\"/content/final_model_dqn.pth\")\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "runner = Runner(model,0,\"/content/save\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "oRxbx_VGktS3",
    "outputId": "5ba07898-85b2-493d-eb34-0359659d32d4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Player_A': 1000, 'Player_B': 0}"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "runner.arena()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "jlXftOLAKW_4",
    "fABA6t4wmRTV",
    "Gr61vkRO8Inb"
   ],
   "machine_shape": "hm",
   "name": "RL_Games_Final.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
